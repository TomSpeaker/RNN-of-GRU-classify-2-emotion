import json
import os
import numpy as np
import pickle
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from model import build_model  # ä» model.py å¯¼å…¥æ¨¡å‹æ„å»ºå‡½æ•°

# ------------------- æ•°æ®åŠ è½½ -------------------
def load_data(json_path):
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    texts = [item['text'] for item in data]
    labels = [item['label'] for item in data]
    return texts, labels

train_texts, train_labels = load_data("train.json")
test_texts, test_labels = load_data("test.json")

# ------------------- æ–‡æœ¬é¢„å¤„ç† -------------------
# Tokenizer æ–‡ä»¶è·¯å¾„
tokenizer_path = "tokenizer.pkl"

# åˆ¤æ–­æ˜¯å¦å­˜åœ¨åˆ†è¯å™¨
if os.path.exists(tokenizer_path):
    print("ğŸ“¦ åŠ è½½å·²æœ‰åˆ†è¯å™¨ tokenizer.pkl")
    with open(tokenizer_path, "rb") as f:
        tokenizer = pickle.load(f)
else:
    print("ğŸ› ï¸ æ²¡æœ‰æ‰¾åˆ°åˆ†è¯å™¨ï¼Œå¼€å§‹è®­ç»ƒæ–°çš„ Tokenizer")
    tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
    tokenizer.fit_on_texts(train_texts)
    
    # ä¿å­˜åˆ†è¯å™¨
    with open(tokenizer_path, "wb") as f:
        pickle.dump(tokenizer, f)
    print("âœ… æ–°åˆ†è¯å™¨å·²ä¿å­˜åˆ° tokenizer.pkl")

train_seqs = tokenizer.texts_to_sequences(train_texts)
test_seqs = tokenizer.texts_to_sequences(test_texts)

max_len = 200
train_pad = pad_sequences(train_seqs, maxlen=max_len, padding='post', truncating='post')
test_pad = pad_sequences(test_seqs, maxlen=max_len, padding='post', truncating='post')

train_labels = np.array(train_labels)
test_labels = np.array(test_labels)

# ------------------- æ¨¡å‹å‡†å¤‡ -------------------
weights_path = "gru_weights.h5"
model = build_model(input_dim=10000, output_dim=128, input_length=max_len)

if os.path.exists(weights_path):
    print("ğŸ” å·²æ£€æµ‹åˆ°æ¨¡å‹æƒé‡ï¼ŒåŠ è½½ä¸­â€¦â€¦")
    model.load_weights(weights_path)
else:
    print("ğŸš€ æœªæ£€æµ‹åˆ°æ¨¡å‹æƒé‡ï¼Œåˆå§‹åŒ–æ–°æ¨¡å‹")

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# ------------------- æ¨¡å‹è®­ç»ƒ -------------------
model.fit(train_pad, train_labels, epochs=5, batch_size=64, validation_data=(test_pad, test_labels))

# ------------------- ä¿å­˜æƒé‡ -------------------
model.save_weights(weights_path)
print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜æƒé‡åˆ°:", weights_path)
